<!DOCTYPE html>

<html>	
<title> Data Excellence Workshop </title>

<body bgcolor="ffffff" style = "font-family:arial;font-size:14px;font-style:normal;">
  
<h1>1st Data Excellence Workshop (DEW2020)</h1>
<p>at <a href=https://www.humancomputation.com/> The eight AAAI Conference on Human Computation and Crowdsourcing </a></p>
<p><b>Date:</b> 26 October 2020 </p>
<p><b>Location:</b> Virtual</p>
<p><b> <a href=http://eval.how/dew-2020> Workshop Website</a></b></p>

<h2> <a href=http://eval.how/dew-2020/program.html> Program </a></h2>
<h2> <a href=http://eval.how/dew-2020/keynotes.html> Keynotes </a></h2>
<h2> <a href="https://easychair.org/conferences/?conf=dew2020"> Submission Link </a></h2>

<h2> Important Dates </h2>
<p><b> Submission Deadline: September 5, 2020 </b></p>
<p><b> Notification of acceptance: September 25, 2020 </b></p>
<p><b> Final camera-ready papers due: October 12, 2020 </b></p>

<h2> Topics of Interest </h2>
We invite scientific contributions and positions papers on the following topics: 

<p><b>Maintainability</b>: Maintaining data at scale, e.g., Knowledge Graph [Bollacker et al, 2008] has similar challenges to maintaining software at scale, or potentially knowledge engineering. What lessons for maintaining data at scale could we learn or adapt from software and knowledge engineering at scale? Data engineering often refers to data munging tasks, and is far more important and challenging that has been appreciated thus far. The fastest thing to do isn’t the most maintainable or reusable. By analogy, data should be well documented [Gebru, 2018], have clear owners/maintainers, should not fork or replicate existing data without documented rationale, should follow shared best practices. 
<ul>
  <li>Data should be not abandoned, should not be disposable</li>
  <li>What if it is a one-off dataset?</li>
    <ul>      
      <li>There is no good software that is abandoned.</li> 
      <li>Even one off dataset require maintenance over time, it needs extension to other languages, addition of new word pairs, updates, scrutiny</li>
      <li>How do we help other people to contribute to growing and improving this data</li>
    </ul>      
  <li>Open infrastructure for supporting maintenance and growth</li>
</ul>
</p>

<p><b>Reliability:</b> Reliability captures internal aspects of validity of the data, such as: consistency, replicability, reproducibility of data. Irreproducible data allows us to draw whatever conclusions we want to draw, while giving us the facade of being data-driven, when it is dangerously hunch driven. So surely we want data to be reproducible [Paritosh, 2012]. 
<ul>
  <li>Irreproducible data is not maintainable</li>
  <li>Reliable mechanisms to account for the human aspects of data collection</li>
  <li>Gathering data from humans requires acknowledging our humanity: frailty, subjectiveness, imperfections, knowledge gaps, etc.</li>
</ul>
</p>

<p><b>Validity:</b> Validity tells us about how well the data helps us explain things related to the phenomena captured by the data, e.g., via correlation between the data and external measures. Education research tries to explore whether grades are valid by studying its correlations to external indicators of student outcome. One thing is clear: you cannot validate the data by itself!
<ul>
  <li>Does the specific operationalization of a data collection procedure (e.g., answering a certain question on a 5-point Likert scale) accurately represent and allow generalizations to the abstract construct that is being modeled (e.g., human sentiment reactions to sentences)?</li>
  <li>Was the construct properly defined before it was operationalized (e.g., getting convicted in court does not equal having committed the crime, but rather the jury's assessment that one committed the crime)?</li>
  <li>Does the data operationalization account for potential complexity, subjectivity, multi-valence or ambiguity of the intended construct?</li>
  <li>Does the data operationalization predict features of the represented construct (e.g., education research tries to explore whether grades are valid by studying its correlations to external indicators of success)?</li>
</ul>
</p>

<p><b>Fidelity:</b> The dataset often represents a phenomenon, e.g., a corpus of sentences and their judged sentiments represents the phenomenon of human sentiment reaction to sentences. A classifier trained on this data will then produce a score which can be used by a product to determine whether to promote or demote a news story on its feed. The users of data often assume that the dataset accurately and comprehensively represents the phenomenon, which is almost never the case. For example, if the sentences in the sentiment corpus were sampled from Wikipedia, it might not work as well as news headlines required by the product. 
<ul>
  <li>Balanced corpus</li>
  <li>Sampling, e.g:</li>
  <ul>
    <li>Temporal splitting can introduce bias if not done right in cases such as signal processing or sequential learning.</li>
    <li>User based splitting not keeping data of users separated is a potential bias source. E.g. if data from the same user is in test and train sets.</li>
  </ul>
  <li>Not a property of a single dataset but a set of them?</li>
  <li>Effort towards fidelity</li>
  <li>Grounding in prior research, science behind the data?</li>
  <li>How good of science can the dataset support?</li>
  <li>Correlation between the data and external measures</li>
  <li>Education research tries to explore whether grades are valid by studying its correlations to external indicators of success?</li>
  <li>Wordnet reaction time study</li>
  <li>You cannot validate the data by itself</li>

<h2> Program Committee </h2>
TBD
</ul>

<h2> Organizing Committee </h2>
<ul>
<li>Praveen Paritosh, Google</li>
<li>Matt Lease, Amazon and UT Austin</li>
<li>Mike Schaekermann, University of Waterloo</li>
<li>Lora Aroyo, Google</li>

</ul>
Email: data_excellence AT googlegroups.com
</body>
</html>
© 2020 GitHub, Inc.
Terms
Privacy
Security
Status
Help
Contact GitHub
Pricing
API
Training
Blog
About
