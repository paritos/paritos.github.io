<!DOCTYPE html>

<html>	
<title> Data Excellence and Rigorous Evaluation for AI/ML</title>

<body bgcolor="#FFFFFF" style="font-family:arial" style="font-size:14px">
<h1>Data Excellence and Rigorous Evaluation for AI/ML</h1>
    
<p> The efficacy of machine learning (ML) models depends on both algorithms and data. Training data defines what we want our models to learn, and testing data provides the means by which their empirical progress is measured.  Benchmark datasets define the entire world within which models exist and operate, yet research continues to focus on critiquing and improving the algorithmic aspect of the models rather than critiquing and improving the data with which our models operate. If “data is the new oil,” we are still missing work on the refineries by which the data itself could be optimized for more effective use.</p>

<p> Measurement of AI success today is often metrics-driven, with emphasis on rigorous model measurement and A/B testing. However, measuring the goodness of the fit of the model to the dataset ignores any consideration of how well the dataset fits the real world problem. Goodness-of-fit metrics, such as F1, Accuracy, AUC, do not tell us much about data fidelity (i.e., how well the dataset represents reality) and validity (how well the data explains things related to the phenomena captured by the data). No standardised metrics exist today for characterising the goodness-of-data. </p>

<p>This page gathers resources, workshops, challenges, and communities focused on data for building and testing AI/ML systems.</p>


<h2>Scientific Workshops </h2>
<ul style="list-style-type:disc">
  <li><a href="http://datacentricai.org/"> NeurIPS Data Centric AI Workshop 2021 </a></li>
  <li><a href="dew2020/index.html">Data Excellence Workshop (DEW) at HCOMP 2020 </a>, (<a href="https://underline.io/events/27/sessions/692/lecture/4760-workshop-1-data-excellence-(dew-2020)">video</a>), (<a href="https://github.com/paritos/paritos.github.io/blob/master/DEW2020%20Master%20Slide%20Deck.pdf">slides</a>), (<a href="https://github.com/paritos/paritos.github.io/blob/master/DEW2020%20Summary%20images.pdf">visual summaries</a>)</li>
  <li><a href="aaai-2020/index.html">Evaluating Evaluation of AI Systems (META-EVAL) AAAI 2020 </a></li>
  <li><a href="reais-2020/index.html">Rigorous Evaluation of AI Systems (REAIS) HCOMP 2020 </a></li>
  <li><a href="reais-2019.html">Rigorous Evaluation of AI Systems (REAIS) HCOMP 2019 </a></li>
  <li><a href="https://sadworkshop.wordpress.com/">Subjectivity, Ambiguity, and Disagreement in Data (SAD) WWW 2019 </a></li>
  <li><a href="https://sadworkshop.wordpress.com/2018-edition/">Subjectivity, Ambiguity, and Disagreement in Data (SAD) WWW 2018 </a></li>
 </ul>
  
<h2>Resources </h2>
<ul style="list-style-type:disc">
  <li><a href="https://ai.googleblog.com/2021/02/uncovering-unknown-unknowns-in-machine.html"> Uncovering Unknown Unknowns in Machine Learning (2021) </a></li>
  <li><a href="https://ojs.aaai.org/index.php/HCOMP/article/download/18941/18706">Iterative Human-in-the-Loop Discovery of Unknown Unknowns in Image Datasets (2021)</a></li>
  <li><a href="https://aclanthology.org/2021.acl-long.548/">Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability (2021)</a></li>
  <li><a href="https://arxiv.org/abs/2103.09058">Understanding the Representation and Representativeness of Age in AI Data Sets (2021)</a></li>
  <li><a href="https://ojs.aaai.org/index.php/HCOMP/article/view/5285">Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance (2019)</a></li>
 </ul>
 

Please fill out this one minute <a href="https://docs.google.com/forms/d/1e-ZkasJ2Xpl-MD5Msi0-5cQru05Zid2GOoCENCWqibk/edit?ts=5fd2df15&gxids=7628">data interest survey</a> if you are interested in this community and join the mailing list. 
<br><br><br>
<b>Subscribe to the <a href="https://groups.google.com/g/data-excellence">data excellence mailing list</a>, or email data-excellence+subscribe AT googlegroups.com</b>
</body>
</html>
