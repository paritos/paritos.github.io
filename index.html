<!DOCTYPE html>

<html>	
<title> Rigorous Evaluation of AI Systems 2019 Workshop at AAAI HCOMP </title>

<body>
<h1>Rigorous Evaluation of AI Systems</h1>
<p>Workshop at AAAI HCOMP 2019 </p>

<p>The last decade has seen massive progress in AI research powered by crowdsourced datasets and benchmarks such as Imagenet, Freebase, SQuAD; as well as widespread adoption and increasing use of AI in deployed systems. A crucial ingredient is the role of crowdsourcing in operationalizing empirical ways for evaluating, comparing, and assessing the progress.</p>
<p> The focus of this workshop is not on evaluating AI systems, but on evaluating the quality of evaluations of AI systems. When these evaluations rely on crowdsourced datasets or methodologies, we are interested in the meta-questions around characterization of those methodologies. Some of the expected activities in the workshop include:
<ul> 
	<li>Asking the question of "what makes evaluations good'?</li>
<li>Defining "what good looks like" in evaluations of different types of AI systems (image recognition, recommender systems, search, voice assistants, etc).</li>
<li>Collecting, examining and sharing current evaluation efforts, comprehensive of one system or competitive of multiple systems with the goal of critically evaluating the evaluations themselves</li>
<li>Developing an open repository of existing evaluations with methodology fully documented and raw data and outcomes available for public scrutiny</li>

</body>
</html>
