<!DOCTYPE html>

<html>	
<title> Rigorous Evaluation of AI Systems 2019 Workshop at AAAI HCOMP </title>

<body>
<h1>Rigorous Evaluation of AI Systems</h1>
<p>Workshop at <a href=https://www.humancomputation.com/> The seventh AAAI Conference on Human Computation and Crowdsourcing </a></p>
<p> Submission Deadline: September 8, 2018 </p>
<p>The last decade has seen massive progress in AI research powered by crowdsourced datasets and benchmarks such as Imagenet, Freebase, SQuAD; as well as widespread adoption and increasing use of AI in deployed systems. A crucial ingredient is the role of crowdsourcing in operationalizing empirical ways for evaluating, comparing, and assessing the progress.</p>
<p> The focus of this workshop is not on evaluating AI systems, but on evaluating the quality of evaluations of AI systems. When these evaluations rely on crowdsourced datasets or methodologies, we are interested in the meta-questions around characterization of those methodologies. Some of the expected activities in the workshop include:
<ul> 
<li>Asking the question of "what makes evaluations good'?</li>
<li>Defining "what good looks like" in evaluations of different types of AI systems (image recognition, recommender systems, search, voice assistants, etc).</li>
<li>Collecting, examining and sharing current evaluation efforts, comprehensive of one system or competitive of multiple systems with the goal of critically evaluating the evaluations themselves</li>
<li>Developing an open repository of existing evaluations with methodology fully documented and raw data and outcomes available for public scrutiny</li>
</ul>
<p> Using crowdsourced datasets for evaluating AI systemsâ€™ success at tasks such as image labeling and question answering have proven powerful enablers for research.  However, adoption of such datasets is typically driven by the mere existence and size of a dataset without proper scrutiny of its scope, quality, and limitations. While crowdsourcing has enabled a burst of published work on specific problems, determining if that work has resulted in real progress cannot continue without a deeper understanding of how the dataset supports the scientific or performance claims of the AI systems it is evaluating. This workshop will provide a forum for growing our collective understanding of what makes a dataset good, the value of improved datasets and collection methods, and how to inform the decisions of when to invest in more robust data acquisition. </p>

<h2> Program Committee </h2>
<ul>
<li>Matt Lease, UT Austin </li>
<li>Paul Tepper, Nuance</li>
<li>Sid Suri, Microsoft</li>
<li>Danna Gurari, UT Austin</li>
<li>Anbang Xu, IBM</li>
<li>Chris Welty, Google</li>
<li>Lora Aroyo, Google</li>
<li>Omar Alonso, Microsoft</li>
<li>Walter Lasecki, Michigan</li>
<li>Sarah Luger, Orange</li>
<li>Alex Quinn, Purdue</li>
<li>Brad Klingenberg, StitchFix</li>
<li>Ka Wong, Google</li>
<li>Panos Ipeirotis, NYU</li>
</ul>

<h3> Organizing Committee </h3>
<ul>
<li>Praveen Paritosh</li>
<li>Kurt Bollacker</li>
<li>Maria Stone</li>
</ul>
Email: rigorous-evaluation AT googlegroups.com
</body>
</html>
