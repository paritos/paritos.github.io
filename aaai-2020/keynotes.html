<!DOCTYPE html>

<html>	
<title> Evaluating Evaluation of AI Systems (Meta-Eval 2020) </title>

<body bgcolor="ffffff" style = "font-family:arial;font-size:14px;font-style:normal;">
  
<h1>Evaluating Evaluation of AI Systems (Meta-Eval 2020)</h1>
<p>Workshop at <a href=https://aaai.org/Conferences/AAAI-20/> The thirty fourth AAAI Conference on Artificial Intelligence </a></p>
<p><b>Date:</b> 8 Feb 2020 </p>
<p><b>Location:</b> Hilton New York Midtown (Conference Venue)</p>
<p><b> <a href=http://eval.how/aaai-2020/> Workshop Website</a></b></p>

<h2> Keynote Speakers </h2>
<h3> <a href=https://www.cs.uml.edu/~arogers/>Anna Rogers</a></h3>
<p><img src="anna_rogers.jpg" alt="Anna Rogers IMG" style="width:100px;height:130px;"></p>
<ul>
<li> <b>Title:</b> The questions that the current AI can't answer. </li>
<li> <b>Abstract:</b> NLP is witnessing an explosion of question answering datasets, most of which get "solved" within months of publication. However, that does not necessarily mean that we are making fast progress towards machine language understanding. I discuss the current proposals for making the datasets more difficult, and the ways in which the current deep learning models "cheat", avoiding the complex verbal reasoning we expect them to perform.</li>
<li> <b>Bio:</b> Anna Rogers is a post-doctoral associate at University of Massachusetts (Lowell). Her research focuses on representation learning, natural language understanding, evaluation methodology for NLP, and computational social science. </li>
</ul>

<h3> Speaker 2</h3>
<ul>
<li> <b>Title:</b> </li>
<li> <b>Abstract:</b> </li>
<li> <b>Bio:</b> </li>
</ul>

<h3> Amazon A2I Team </h3>
<ul>
<li> <b>Title:</b> Humans and AI. </li>
<li> <b>Abstract:</b> </li>
<li> <b>Bio:</b> </li>
</ul>



<h2>Contact Organizing Committee </h2>
Email: rigorous-evaluation AT googlegroups.com
</body>
</html>
