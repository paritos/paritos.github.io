<!DOCTYPE html>

<html>	
<title> Evaluating Evaluation of AI Systems (Meta-Eval 2020) </title>

<body bgcolor="ffffff">
  <h1><font face="arial">Evaluating Evaluation of AI Systems (Meta-Eval 2020)</font></h1>
<p>Workshop at <a href=https://aaai.org/Conferences/AAAI-20/> The thirty fourth AAAI Conference on Artificial Intelligence </a></p>
<p><b> Submission Deadline: November 27, 2019 </b></p>
<p><b> Notification of acceptance: December 13, 2019 </b></p>
<p><b> Final camera-ready papers due: December 18, 2019 </b></p>

<p> <a href="https://easychair.org/cfp/reais19"> Call for papers </a></p>
<h2> <a href="https://easychair.org/conferences/?conf=reais19"> Submission Link </a></h2>

<p>The last decade has seen massive progress in AI research powered by crowdsourced datasets and benchmarks such as Imagenet, Freebase, SQuAD; as well as widespread adoption and increasing use of AI in deployed systems. A crucial ingredient is the role of crowdsourcing in operationalizing empirical ways for evaluating, comparing, and assessing the progress.</p>
<p> The focus of this workshop is not on evaluating AI systems, but on evaluating the quality of evaluations of AI systems. When these evaluations rely on crowdsourced datasets or methodologies, we are interested in the meta-questions around characterization of those methodologies. Some of the expected activities in the workshop include:
<ul> 
<li>Asking the question of "what makes evaluations good'?</li>
<li>Defining "what good looks like" in evaluations of different types of AI systems (image recognition, recommender systems, search, voice assistants, etc).</li>
<li>Collecting, examining and sharing current evaluation efforts, comprehensive of one system or competitive of multiple systems with the goal of critically evaluating the evaluations themselves</li>
<li>Developing an open repository of existing evaluations with methodology fully documented and raw data and outcomes available for public scrutiny</li>
</ul>
<p> Using crowdsourced datasets for evaluating AI systems’ success at tasks such as image labeling and question answering have proven powerful enablers for research.  However, adoption of such datasets is typically driven by the mere existence and size of a dataset without proper scrutiny of its scope, quality, and limitations. While crowdsourcing has enabled a burst of published work on specific problems, determining if that work has resulted in real progress cannot continue without a deeper understanding of how the dataset supports the scientific or performance claims of the AI systems it is evaluating. This workshop will provide a forum for growing our collective understanding of what makes a dataset good, the value of improved datasets and collection methods, and how to inform the decisions of when to invest in more robust data acquisition. </p>

<h2> Topics of Interest </h2>

We invite scientific contributions and positions papers on the following topics: 
<ul>

<li>META-EVALUATION: Quality of evaluation approaches, datasets / benchmarks</li> 
<ul>
<li>Characteristics of ‘good’ dataset / benchmark?</li>
<li>Shortcomings of existing evaluation approaches, datasets / benchmarks?</li>
<li>Building new / improving existing metrics</li>
<li>Measuring trustworthiness, interpretability and fairness of crowdsourced benchmarks datasets</li>
<li>Measuring added value of improvements to previous versions of benchmark datasets</li>
<li>Comparative evaluations between mainstream AI systems, e.g. recommenders, voice assistants, etc.</li>
<li>Measuring quality of guidelines for content moderation, search evaluation, etc.</li>
<li>Comparison of results between offline (e.g. crowdsourced) and online (e.g. A/B testing) evaluations?</li> 
<li>Open questions and challenges in meta-evaluation?</li>
</ul>

<li>TRANSPARENCY: Making quality and characteristics of (crowdsourced) benchmark datasets transparent and explainable</li>
<ul>
<li>Reproducibility of crowdsourced datasets</li>
<li>Replicability of crowdsourced evaluations of AI systems</li>
<li>Explainability of crowdsourced evaluations to different stakeholders, e.g. users, scientists, developers</li>
</ul>
  
<li> RESOURCE BUILDING: Making existing evaluation methodologies, raw data and outcomes, discoverable, fully documented and available for public scrutiny
<ul>
<li>How do we make evaluations and related datasets archival and discoverable?</li>
<li>What can we learn from other systematic evaluation efforts and communities such as TREC, SIGIR, etc.?</li>
</ul>
</ul>
<h2> Program Committee </h2>
<ul>
<li>Matt Lease, UT Austin </li>
<li>Paul Tepper, Nuance</li>
<li>Sid Suri, Microsoft</li>
<li>Danna Gurari, UT Austin</li>
<li>Anbang Xu, IBM</li>
<li>Chris Welty, Google</li>
<li>Lora Aroyo, Google</li>
<li>Omar Alonso, Microsoft</li>
<li>Walter Lasecki, Michigan</li>
<li>Sarah Luger, Orange</li>
<li>Alex Quinn, Purdue</li>
<li>Brad Klingenberg, StitchFix</li>
<li>Ka Wong, Google</li>
<li>Panos Ipeirotis, NYU</li>
</ul>

<h3> Organizing Committee </h3>
<ul>
<li>Praveen Paritosh, Google</li>
<li>Kurt Bollacke, Long Now Foundationr</li>
<li>Maria Stone, Spotify</li>
<li>Lora Aroyo, Google</li>
<li>Sarah Luger, Orange</li>
</ul>
Email: rigorous-evaluation AT googlegroups.com
</body>
</html>
