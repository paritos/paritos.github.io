<!DOCTYPE html>

<html>	
<title> Evaluating Evaluation of AI Systems (Meta-Eval 2020) </title>

<body bgcolor="ffffff" style = "font-family:arial;font-size:14px;font-style:normal;">
  
<h1>Evaluating Evaluation of AI Systems (Meta-Eval 2020)</h1>
<p>Workshop at <a href=https://aaai.org/Conferences/AAAI-20/> The thirty fourth AAAI Conference on Artificial Intelligence </a></p>
<p><b>Date:</b> 8 Feb 2020 </p>
<p><b>Location:</b> Hilton New York Midtown (Conference Venue)</p>
<p><b> <a href=http://eval.how/aaai-2020/> Workshop Website</a></b></p>

<h2> Program </h2>
<ul>
<li>9:15 - 9:30 Intro</li>
  <li><b>9:30 - 10:30 Invited Talk: <a href=http://eval.how/aaai-2020/keynotes.html>Anna Rogers</a></b></li> 
<li>10:30 - 11:00 Coffee Break</li>
  <li><b>11:00 - 12:00 Paper Session 1</b></li>
<ul>
<li>11:00 - 11:15: Jose Hernandez-Orallo: <b><a href=http://eval.how/aaai-2020/REAIS19_p14.pdf>AI Evaluation: On Broken Yardsticks and Measurement Scales</a></b></li>
<li>11:15 - 11:30: Christopher Pereyda and Lawrence Holder: <b><a href=http://eval.how/aaai-2020/REAIS19_p8.pdf>Measuring the Relative Similarity and Difficulty Between AI Benchmark Problems</a></b></li>
<li>11:30 - 11:45: Azamat Kamzin, Prajwal Paudyal, Ayan Banerjee and Sandeep K.S. Gupta: <b><a href=http://eval.how/aaai-2020/REAIS19_p13.pdf>Evaluating the gap between hype and performance of AI systems</a></b></li>
<li>11:45 - 12:00: Chris Welty, Praveen Paritosh, Lora Aroyo: <b><a href=https://arxiv.org/abs/1911.01875>Metrology for AI: From Benchmarks to Instruments</a></b></li>
</ul>
  <li><b>12:00 - 13:00 Invited Talk: <a href=http://eval.how/aaai-2020/keynotes.html>TBC </a></b></li>
<li>13:00 - 14:00 Lunch</li>
  <li><b>14:00 - 14:30 Paper Session 2</b></li>
<ul>
<li>14:00 - 14:15: Botty Dimanov, Umang Bhatt, Mateja Jamnik and Adrian Weller: <b><a href=http://eval.how/aaai-2020/REAIS19_p7.pdf>You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods</a></b></li>	
<li>14:15 - 14:30: Inioluwa Raji and Genevieve Fried: <b><a href=http://eval.how/aaai-2020/REAIS19_p12.pdf>About Face: Tracking Shifting Trends of Facial Recognition Evaluation</a></b></li>
</ul>
  <li><b>14:30 - 15:30 Invited Talk: <a href=http://eval.how/aaai-2020/keynotes.html>Amazon A2I Team - Humans and AI</a></b></li>
<li>15:30 - 16:00 Coffee break (Town Hall meeting)</li>
  <li><b>16:00 - 16:45 Paper Session 3</b></li>
<ul>
<li>16:00 - 16:15: Stefano Alletto, Shenyang Huang, Vincent Francois-Lavet, Yohei Nakata and Guillaume Rabusseau: <b><a href=http://eval.how/aaai-2020/REAIS19_p9.pdf>RandomNet: Towards Fully Automatic Neural Architecture Design for Multimodal Learning</a></b></li>
<li>16:15 - 16:30: Julian Niedermeier, Goncalo Mordido and Christoph Meinel: <b><a href=http://eval.how/aaai-2020/REAIS19_p10.pdf>Improving the Evaluation of Generative Models with Fuzzy Logic</a></b></li>
<li>16:30 - 16:45: Huiyuan Xie, Tom Sherborne, Alexander Kuhnle and Ann Copestake: <b><a href=http://eval.how/aaai-2020/REAIS19_p11.pdf>Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity</a></b></li>
</ul>
  <li><b>16:45 - 17:30 Panel: Perspectives on Self-Evaluation</b>: Should researchers, groups, companies evaluate their own work, or is a more adversarial approach necessary to achieve evaluation quality?</li>
  <ul>
    <li>Chair: Chris Welty (Google)</li>
    <li>Participants: [TBA]</li>
    <li>Topics: [TBA] </li>
  </ul>

</ul>




<h2>Contact Organizing Committee </h2>
Email: rigorous-evaluation AT googlegroups.com
</body>
</html>
