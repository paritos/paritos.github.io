<!DOCTYPE html>

<html>	
<title> Evaluating Evaluation of AI Systems (Meta-Eval 2020) </title>

<body bgcolor="ffffff" style = "font-family:arial;font-size:14px;font-style:normal;">
  
<h1>Evaluating Evaluation of AI Systems (Meta-Eval 2020)</h1>
<p>Workshop at <a href=https://aaai.org/Conferences/AAAI-20/> The thirty fourth AAAI Conference on Artificial Intelligence </a></p>
<p><b>Date:</b> 8 Feb 2020 </p>
<p><b>Location:</b> Hilton New York Midtown (Conference Venue)</p>
<p><b> <a href=http://eval.how/aaai-2020/> Workshop Website</a></b></p>

<h2> Program </h2>
<ul>
<li>9:15 - 9:30 Intro</li>
<li>9:30 - 10:30 Invited Talk: Anna Rogers</li> 
<li>10:30 - 11:00 Coffee Break</li>
<li>11:00 - 12:00 Paper Session 1</li>
<ul>
<li>11:00 - 11:15: Jose Hernandez-Orallo: AI Evaluation: On Broken Yardsticks and Measurement Scales</li>
<li>11:15 - 11:30: Christopher Pereyda and Lawrence Holder: Measuring the Relative Similarity and Difficulty Between AI Benchmark Problems</li>
<li>11:30 - 11:45: Azamat Kamzin, Prajwal Paudyal, Ayan Banerjee and Sandeep K.S. Gupta: Evaluating the gap between hype and performance of AI systems</li>
<li>11:45 - 12:00: Chris Welty, Praveen Paritosh, Lora Aroyo: Metrology for AI: From Benchmarks to Instruments</li>
</ul>
<li>12:00 - 13:00 Invited Talk: </li>
<li>13:00 - 14:00 Lunch</li>
<li>14:00 - 14:30 Paper Session 2</li>
<ul>
<li>14:00 - 14:15: Botty Dimanov, Umang Bhatt, Mateja Jamnik and Adrian Weller: You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods</li>	
<li>14:15 - 14:30: Inioluwa Raji and Genevieve Fried: About Face: Tracking Shifting Trends of Facial Recognition Evaluation</li>
</ul>
<li>14:30 - 15:30 Invited Talk: Amazon A2I Team - Humans and AI</li>
<li>15:30 - 16:00 Coffee break (Town Hall meeting)</li>
<li>16:00 - 16:45 Paper Session 3</li>
<ul>
<li>16:00 - 16:15: Stefano Alletto, Shenyang Huang, Vincent Francois-Lavet, Yohei Nakata and Guillaume Rabusseau: RandomNet: Towards Fully Automatic Neural Architecture Design for Multimodal Learning</li>
<li>16:15 - 16:30: Julian Niedermeier, Goncalo Mordido and Christoph Meinel: Improving the Evaluation of Generative Models with Fuzzy Logic</li>
<li>16:30 - 16:45: Huiyuan Xie, Tom Sherborne, Alexander Kuhnle and Ann Copestake: Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity</li>
</ul>
<li>16:45 - 17:30 Panel: Industry Perspectives</li>
</ul>




<h2>Contact Organizing Committee </h2>
Email: rigorous-evaluation AT googlegroups.com
</body>
</html>
