<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <!--Favicon-->
  <link rel="shortcut icon" href="/reais-2020/assets/favicon.ico" type="image/x-icon">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.9.0/css/all.css"
    integrity="sha384-i1LQnF23gykqWXg6jxC2ZbCbUMxyw5gLZY6UiUS98LYV5unm8GWmfkIS6jqJfb4E" crossorigin="anonymous">

  <!-- Spoqa Han Sans -->
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/reais-2020/assets/css/main.css">

  <!-- OG Tag -->
  
  <meta name="title" content="REAIS Workshop at HCOMP 2020" />
  <meta name="author" content="Workshop on Rigorous Evaluation for AI Systems" />
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta name="robots" content="index,follow" />

  <meta property="og:title" content="REAIS Workshop at HCOMP 2020" />
  <meta property="og:description" content="" />
  <meta property="og:type" content="website, blog" />
  <meta property="og:image"
    content="http://eval.how/assets/img/logo.png" />
  <meta property="og:site_name" content="REAIS Workshop at HCOMP 2020" />
  <meta property="og:url" content="http://eval.how/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="REAIS Workshop at HCOMP 2020" />
  <meta name="twitter:description" content="" />
  <meta name="twitter:image"
    content="http://eval.how/assets/img/logo.png" />

  <title>REAIS Workshop at HCOMP 2020</title>
</head>
<body>
  <div class="container">
    

<header>
  <nav>
    <ul>
      
      <a href="http://eval.how/reais-2020/">
        <li class="current btn-nav">Home</li>
      </a>
      <a href="http://eval.how/reais-2020/program.html">
        <li class="btn-nav">Program</li>
      </a>
      <!--<a href="http://eval.how/reais-2020/portfolio">
        <li class="btn-nav">Portfolio</li>
      </a>-->
      
    </ul>
  </nav>
</header>



</div><!-- This Tag means End .container -->
<div id="protfolio">
  <section id="about-me">
    <div class="row">
      <div class="img-wrap col-12 col-md-4">
        <div class="img">
          <img src="/reais-2020/assets/img/logo.png" alt="user_image">
        </div>
      </div>
      <div class="text-wrap col-12 col-md-8">
        <h2 class="username">Workshop on Rigorous Evaluation for AI Systems</h2>
        <p class="description">Virtual meeting on Sunday, October 25, 2020. Part of the AAAI Conference on Human Computation and Crowdsourcing.</p>

        <ul id="icons">
          

<li>
  <a href="mailto:rigorous-evaluation@googlegroups.com">
    <i class="fas fa-envelope"></i>
  </a>
</li>













<li>
  <a href="https://twitter.com/hcomp_conf">
    <i class="fab fa-twitter"></i>
  </a>
</li>







        </ul>
      </div>
    </div>
    <div class="bg"></div>
  </section>
</div>

<div id="post">
  <section class="post-header">
    <h1 class="title"></h1>
    <p class="subtitle"></p>
    <p class="meta">
      
    </p>
  </section>
  <section class="post-content">
    <h1 id="rigorous-evaluation-for-ai-systems-workshop">Rigorous Evaluation for AI Systems Workshop</h1>

<p>This year, the REAIS workshop will run a joint program with DEW 2020 across two days:</p>
<ul>
  <li>October 25, 2020: REAIS 2020 (on this page)</li>
  <li>October 26, 2020: <a href="http://eval.how/dew2020/">DEW 2020</a></li>
</ul>

<p><em>Submission link: <a href="https://easychair.org/conferences/?conf=reais2020">https://easychair.org/conferences/?conf=reais2020</a></em></p>

<p>The last decade has seen massive progress in AI research powered by crowdsourced datasets and benchmarks such as ImageNet, Freebase, SQuAD; as well as widespread adoption and increasing use of AI in deployed systems. A crucial ingredient is the role of crowdsourcing in operationalizing empirical ways for evaluating, comparing, and assessing the progress.</p>

<p>Using crowdsourced techniques for evaluating AI systems’ success at tasks such as image labeling and question answering have proven powerful enablers for research. However, adoption of such approaches is typically driven by the mere existence and size of crowdsourced contributions without proper scrutiny of their scope, quality, and limitations. While crowdsourcing has enabled a burst of published work on specific problems, determining if that work has resulted in real progress cannot continue without a deeper understanding of how dataset and benchmarks support the scientific or performance claims of the AI systems it is evaluating. This workshop will provide a forum for growing our collective understanding of what makes an evaluation good, the value of improved datasets and collection methods, and how to inform the decisions of when to invest in more robust data acquisition.</p>

<h2 id="this-years-focus-third-party-and-independent-meta-evaluation">This year’s focus: Third Party And Independent Meta-evaluation</h2>

<p>Often AI systems and datasets are evaluated by measures that only have mathematical and theoretical components, while the real, physical world is messy, irregular, and subject to constant change.  Some systems and approaches that perform well under the scrutiny of clean, elegant theory may still fail quite spectacularly in real-world applications simply because the theory did not match reality.  Traditionally, approaches such as crowdsourcing, “human in the loop” decision systems, HCI mechanisms, and other human-centered sources of ground truth are powerful complements used to fill in that “real-world” gap.   However, because they are often part of the same construction by the same creators, this human component of measure and evaluation has the correlated biases and omissions as the rest of the system.  To overcome these biases and omissions, an independent layer of 3rd party human-centered evaluation, or “meta-evaluation” may be needed.  If this separate, external scrutiny is crafted from the perspective of actual users and consumers of such datasets and systems rather than the expectations of the system/dataset creators, it might be used to more accurately measure real-world performance and value.</p>

<p>This year, we will will have a focus on how we can use human computation to craft external, independent evaluation of AI datasets and systems, especially focussed on application to:</p>

<ul>
  <li>Building the appropriate level of trust and confidence, especially for systems that have direct safety or economic impact on people</li>
  <li>Providing guidance for choosing between or making resource commitments to real world instances of systems</li>
  <li>Detecting fraudulent claims about systems and manipulation of data</li>
  <li>Compensating for gaming, deception, and other malicious use of AI systems</li>
  <li>Constructing adversarial scrutiny of datasets and systems</li>
</ul>

<h2 id="key-dates">Key Dates</h2>
<s>October 14, 2020: Extended abstracts and short papers due</s>
<s>October 18, 2020: Notification of acceptance</s>
<p><strong>October 25, 2020: Rigorous Evaluation for AI Systems workshop</strong><br />
October 26, 2020: sister workshop, Data Excellence Workshop (<a href="http://eval.how/dew2020">link</a>)</p>

<p>Workshop Authors are invited to submit extended abstracts (2-4 pages) or short papers (4-6 pages), plus any number of additional pages containing references only.</p>

<h2 id="organizing-committee">Organizing committee</h2>
<p>Bernease Herman<br />
Sarah Luger<br />
Kurt Bollacker<br />
Maria Stone</p>

<h2 id="venue">Venue</h2>
<p>The workshop will be held at the eighth AAAI Conference on Human Computation and Crowdsourcing.</p>

<h2 id="contact">Contact</h2>
<p>All questions about submissions should be emailed to rigorous-evaluation@googlegroups.com.</p>

  </section>
</div>

<div id="top" class="top-btn" onclick="moveTop()">
  <i class="fas fa-chevron-up"></i>
</div>

  </div>
</body>

</html>