<h1>Rigorous Evaluation of AI Systems Workshop</h1>
<p><b>AAAI Conference on Human Computing and Crowdsourcing<br>October 26, 2020 (virtual)</b></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">This workshop is part of the HCOMP 2020 conference. We intend to build a community interested in using rigorous evaluation to tackle real-world problems that may be otherwise inaccessible.&nbsp;</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">The last decade has seen massive progress in AI research powered by crowdsourced datasets and benchmarks such as Imagenet, Freebase, SQuAD; as well as widespread adoption and increasing use of AI in deployed systems. A crucial ingredient is the role of crowdsourcing in operationalizing empirical ways for evaluating, comparing, and assessing the progress.&nbsp;&nbsp;</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Using crowdsourced techniques for evaluating AI systems’ success at tasks such as image labeling and question answering have proven powerful enablers for research. However, adoption of such approaches is typically driven by the mere existence and size of crowdsourced contributions without proper scrutiny of their scope, quality, and limitations. While crowdsourcing has enabled a burst of published work on specific problems, determining if that work has resulted in real progress cannot continue without a deeper understanding of how dataset and benchmarks support the scientific or performance claims of the AI systems it is evaluating. This workshop will provide a forum for growing our collective understanding of what makes an evaluation good, the value of improved datasets and collection methods, and how to inform the decisions of when to invest in more robust data acquisition.&nbsp;</span></p>

<p dir="ltr"><strong>This Year’s Focus: Third Party And Independent Meta-evaluation</strong></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Often AI systems and datasets are evaluated by measures that only have mathematical and theoretical components, while the real, physical world is messy, irregular, and subject to constant change.&nbsp; Some systems and approaches that perform well under the scrutiny of clean, elegant theory may still fail quite spectacularly in real-world applications simply because the theory did not match reality.&nbsp; Traditionally, approaches such as crowdsourcing, "human in the loop" decision systems, HCI mechanisms, and other human-centered sources of ground truth are powerful complements used to fill in that "real-world" gap. &nbsp; However, because they are often part of the same construction by the same creators, this human component of measure and evaluation has the correlated biases and omissions as the rest of the system.&nbsp; To overcome these biases and omissions, an independent layer of 3rd party human-centered evaluation, or "meta-evaluation" may be needed.&nbsp; If this separate, external scrutiny is crafted from the perspective of actual users and consumers of such datasets and systems rather than the expectations of the system/dataset creators, it might be used to more accurately measure real-world performance and value.&nbsp;</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">This year, we will will have a focus on how we can use human computation to craft external, independent evaluation of AI datasets and systems, especially focussed on application to:</span></p>

<ul>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Building the appropriate level of trust and confidence, especially for systems that have direct safety or economic impact on people</span></p>
	</li>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Providing guidance for choosing between or making resource commitments to real world instances of systems</span></p>
	</li>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Detecting fraudulent claims about systems and manipulation of data</span></p>
	</li>
	<li dir="ltr" style="list-style-type:disc">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Compensating for gaming, deception, and other malicious use of AI systems</span></p>
	</li>
	<li dir="ltr" style="list-style-type:disc">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Constructing adversarial scrutiny of datasets and systems&nbsp;</span></p>
	</li>
</ul>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">In the context of these applications, we invite scientific contributions and positions papers on the following topics:</span></p>

<ul>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><strong>META-EVALUATION:</strong><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt"> Quality of evaluation approaches, datasets / benchmarks&nbsp; </span><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">&nbsp;&nbsp;&nbsp; </span></p>

	<ul>
		<li style="list-style-type:circle" dir="ltr">
		<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Characteristics of ‘good’ dataset / benchmark?</span></p>
		</li>
		<li dir="ltr" style="list-style-type:circle">
		<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Shortcomings of existing evaluation approaches, datasets / benchmarks?</span></p>
		</li>
		<li style="list-style-type:circle" dir="ltr">
		<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Building new / improving existing metrics</span></p>
		</li>
		<li style="list-style-type:circle" dir="ltr">
		<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Measuring trustworthiness, interpretability and fairness of crowdsourced benchmarks&nbsp;</span></p>
		</li>
		<li style="list-style-type:circle" dir="ltr">
		<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Measuring added value of improvements to previous versions of benchmark datasets</span></p>
		</li>
		<li style="list-style-type:circle" dir="ltr">
		<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Comparison of results between offline (e.g. crowdsourced) and online (e.g. A/B testing) evaluations?&nbsp;</span></p>
		</li>
	</ul>
	</li>
</ul>

<ul>
	<li dir="ltr" style="list-style-type:disc">
	<p dir="ltr"><strong>AVAILABILITY:</strong><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt"> Making quality and characteristics of (crowdsourced) benchmarks, datasets, and systems&nbsp; explainable, discoverable, fully documented and available for public scrutiny&nbsp;</span></p>

	<ul>
		<li dir="ltr" style="list-style-type:circle">
		<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Replicability of crowdsourced evaluations of AI systems</span></p>
		</li>
		<li style="list-style-type:circle" dir="ltr">
		<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Explainability of crowdsourced evaluations to different stakeholders, e.g. users, scientists, developers</span></p>
		</li>
		<li dir="ltr" style="list-style-type:circle">
		<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">How do we make evaluations and related datasets archival and discoverable?</span></p>
		</li>
	</ul>
	</li>
</ul>

<h2 dir="ltr">Key Dates</h2>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">&nbsp;&nbsp;&nbsp; </span><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">October 4, 2020: Full papers due</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">&nbsp;&nbsp;&nbsp; </span><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">October 13, 2020: Notification of acceptance</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">&nbsp;&nbsp;&nbsp; </span><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">October 20, 2020: Final camera-ready papers due</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Workshop Authors are invited to submit papers of up to 6 pages, plus any number of additional pages containing references only.</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">All submitted papers must represent original work, not previously published or under simultaneous peer-review for any other peer-reviewed, archival conference or journal.</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Papers must be formatted in AAAI two-column, camera-ready style; please refer to the style guide here:</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">(https://www.aaai.org/Publications/Templates/Original_AAAI_Style.zip) for details. Papers must be in trouble-free, high-resolution PDF format, formatted for US Letter (8.5″ x 11″) paper, using Type 1 or TrueType fonts. The AAAI copyright block is not required on submissions, but must be included on final accepted versions.</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Electronic paper submission through the HCOMP-20 EasyChair paper submission site required on or before the deadlines listed above. We cannot accept submissions by e-mail or fax. Authors will receive confirmation of receipt of their abstracts or papers, including an ID number, shortly after submission. HCOMP will contact authors again only if problems are encountered with papers.</span></p>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">At least one author of each accepted paper must register for the conference to present the work or acceptance will be withdrawn.</span></p>

<h2 dir="ltr">List of Workshop Activities</h2>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">The focus of this workshop is not on evaluating AI systems, but on evaluating the quality of evaluations of AI systems. When these evaluations rely on crowdsourced datasets or methodologies, we are interested in the meta-questions around characterization of those methodologies. Some of the expected activities in the workshop include:</span></p>

<ul>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Asking the question of "what makes evaluations good'?</span></p>
	</li>
	<li dir="ltr" style="list-style-type:disc">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Defining "what good looks like" in evaluations of different types of AI systems (image recognition, recommender systems, search, voice assistants, etc).</span></p>
	</li>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Collecting, examining and sharing current evaluation efforts, comprehensive of one system or competitive of multiple systems with the goal of critically evaluating the evaluations themselves</span></p>
	</li>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Developing an open repository of existing evaluations with methodology fully documented and raw data and outcomes available for public scrutiny</span></p>
	</li>
</ul>

<h2 dir="ltr">Organizing committee</h2>

<ul>
	<li dir="ltr" style="list-style-type:disc">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Bernease Herman</span></p>
	</li>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Sarah Luger</span></p>
	</li>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Kurt Bollacker</span></p>
	</li>
	<li style="list-style-type:disc" dir="ltr">
	<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">Maria Stone</span></p>
	</li>
</ul>

<h2 dir="ltr">Venue</h2>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">The Workshop will be held at The eighth AAAI Conference on Human Computation and Crowdsourcing</span></p>

<h2 dir="ltr">Contact</h2>

<p dir="ltr"><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">All questions about submissions should be emailed to </span><strong>rigorous-evaluation@googlegroups.com</strong><span style="background-color:transparent; color:#000000; font-family:Arial; font-size:12pt">.</span></p></div></div></div></td></tr><tr><td class="footer">Copyright © 2002 – 2020 EasyChair</td></tr></table></body>
</html>
