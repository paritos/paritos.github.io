<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <!--Favicon-->
  <link rel="shortcut icon" href="/reais-2020/assets/favicon.ico" type="image/x-icon">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.9.0/css/all.css"
    integrity="sha384-i1LQnF23gykqWXg6jxC2ZbCbUMxyw5gLZY6UiUS98LYV5unm8GWmfkIS6jqJfb4E" crossorigin="anonymous">

  <!-- Spoqa Han Sans -->
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/reais-2020/assets/css/main.css">

  <!-- OG Tag -->
  
  <meta name="title" content="REAIS Workshop at HCOMP 2020" />
  <meta name="author" content="Workshop on Rigorous Evaluation for AI Systems" />
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta name="robots" content="index,follow" />

  <meta property="og:title" content="REAIS Workshop at HCOMP 2020" />
  <meta property="og:description" content="" />
  <meta property="og:type" content="website, blog" />
  <meta property="og:image"
    content="http://eval.how/assets/img/logo.png" />
  <meta property="og:site_name" content="REAIS Workshop at HCOMP 2020" />
  <meta property="og:url" content="http://eval.how/program.html" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="REAIS Workshop at HCOMP 2020" />
  <meta name="twitter:description" content="" />
  <meta name="twitter:image"
    content="http://eval.how/assets/img/logo.png" />

  <title>REAIS Workshop at HCOMP 2020</title>
</head>
<body>
  <div class="container">
    

<header>
  <nav>
    <ul>
      
      <a href="http://eval.how/reais-2020/">
        <li class="btn-nav">Home</li>
      </a>
      <li class="current">Program</li>
      <!-- <a href="http://eval.how/reais-2020/portfolio">
        <li class="btn-nav">Portfolio</li>
      </a> -->
      
    </ul>
  </nav>
</header>

<h1 id="program-agenda">Program Agenda</h1>
<p>(subject to change)</p>

<p>2:00pm  Tech Platform Onboarding
<br />2:10pm  Introduction
<br />2:25pm  Invited Talk: José Hernández-Orallo
<br />3:05pm  Break
<br />3:15pm  NLP Invited Talks: Ellen Voorhees, João Sedoc, Courtney Napoles
<br />4:45pm  Panel discussion
<br />5:05pm  Break
<br />5:15pm  Reporting from discussions
<br />5:20pm  Invited Talk: Besmira Nushi
<br />6:00pm  Submitted Talks
<br />6:20pm  Closing remarks
<br />
<br /></p>

<h1 id="speakers">Speakers</h1>

<p><strong>José Hernández-Orallo, Professor, Universitat Politècnica de València</strong> (<a href="http://josephorallo.webs.upv.es/">website</a>)</p>

<p><strong>Title:</strong> Adversarial Testing: from Adversarial Datasets to Cognitively-Adaptive Testing</p>

<p><strong>Abstract:</strong> AI evaluation has been fighting a decades-long battle against overfitting, caused by AI researchers designing their systems to the test or training them to the benchmark dataset of collected task instances. Very recently, an adversarial approach has been emerging as a way of collecting examples that take AI systems to their limits, and find their vulnerabilities and limitations. However, this is a very time-consuming effort that relies on human computation, usually through crowdsourcing. We would like this kind of ‘Turing testing’ and ‘adversarial testing’ to become more dynamic and automated. We must recognise that this effort is cognitive and hence hard –but not impossible– to automate. In this talk I discuss some possibilities to automate ‘cognitively-adaptive testing’ based on adversarial (generative) models, and other more general techniques such as computerised adaptive testing, adaptive sampling, active learning, cognitively-precise evaluation, machine teaching, etc.
<br />
<br /></p>

<p><strong>Ellen Voorhees, Senior Research Scientist, NIST</strong> (<a href="https://www.nist.gov/people/ellen-m-voorhees">website</a>)</p>

<p><strong>Title:</strong> Evaluating Messy Tasks</p>

<p><strong>Abstract:</strong> A messy task is one in which humans have valid differences of opinion as to the quality or correctness of a candidate response. Language processing is rife with messy tasks—humans disagree about the relevance of documents retrieved in response to query, the acceptability of answers to a question (even for factoid questions), the quality of a document summary, the appropriateness of a translation, and the helpfulness of an explanation to name a few. Evaluating automated systems for messy tasks is complicated by this lack of a true gold standard.  Human annotators can be trained to converge on a single interpretation of quality, but that often defeats the purpose since eventual end users of the technology will neither know nor care about annotation standards. This talk will review how humans’ inherent disagreement has shaped the evaluation methodologies used to drive technology development for messy language tasks.
<br />
<br /></p>

<p><strong>João Sedoc, Assistant Professor, New York University Stern School of Business</strong> (<a href="https://www.stern.nyu.edu/faculty/bio/joao-sedoc">website</a>)</p>

<p><strong>Title:</strong> Evaluating Conversational Agents</p>

<p><strong>Abstract:</strong> There has been a renewed focus on dialog systems, including non-task driven conversational agents. Dialog is a challenging problem since it spans multiple conversational turns. To further complicate the problem, there are many possible valid utterances that may be semantically different. This makes automatic evaluation difficult, which is why the current best practice for analyzing and comparing dialog systems is the use of human judgments. This talk focuses on evaluation, presenting a theoretical framework for the systematic evaluation of open-domain conversational agents, including the usage of Item Response Theory (Lord, 1968) for efficient chatbot evaluation and evaluation set creation.  We introduce ChatEval <a href="https://chateval.org">https://chateval.org</a> a unified framework for human evaluation of chatbots that augments existing tools and provides a web-based hub for researchers to share and compare their dialog systems.
<br />
<br /></p>

<p><strong>Courtney Napoles, Language Data Manager, Grammarly</strong> (<a href="http://www.courtneynapoles.com">website</a>)</p>

<p><strong>Title:</strong> Evaluating text generation with “gold-standard” labeled data</p>

<p><strong>Abstract:</strong> Like other areas of AI, natural language processing relies on automatic evaluation to support the advancement of new models. Since much automatic evaluation is supervised, text generation poses a particular challenge because there is no single gold answer. Even the highest quality labels provided by highly skilled human annotators are subject to bias and oversights. This talk will consider challenges encountered when evaluating text generation models with gold-standard datasets, including the risks of relying on a human-labeled gold standard, efficiently supporting automatic evaluation with human judgments, and the importance of establishing a feedback loop to reassess the metrics themselves as domains and models change.
<br />
<br /></p>

<p><strong>Besmira Nushi, Senior Researcher, Microsoft Research AI</strong> (<a href="https://besmiranushi.com">website</a>)</p>

<p><strong>Title:</strong> The Unpaved Path of Deploying Reliable and Human-Centered Machine Learning Systems</p>

<p><strong>Abstract:</strong> As machine learning systems are increasingly becoming part of user-facing applications, their reliability and robustness are key to building and maintaining trust with users, especially for high-stake domains such as healthcare. While advances in learning are continuously improving model performance in expectation and in isolation, there is an emergent need for identifying, understanding, and mitigating cases where models may fail in unexpected ways and therefore break human trust or dependencies with other larger software ecosystems. Current development infrastructures and methodologies often designed with traditional software in mind, still provide very little support to enable practitioners debug and troubleshoot systems over time. This discussion will look at such problems from two different stakeholder lenses: machine learning practitioners and end user decision makers. From a practitioner perspective, it will summarize some of the current gaps in tooling for responsible ML development and evaluation, and present ongoing work that can enable in-depth error analysis and careful model versioning. Next, from an end user perspective it will propose rethinking the optimization of machine learning models such that it takes into consideration human-centered properties of human-machine collaboration and partnership. While both these lenses pose both research and engineering practices, they also require close collaboration with domain experts who are using machine learning in the open field to ensure that deployed systems meet real-world expectations.</p>

  </div>
</body>

</html>